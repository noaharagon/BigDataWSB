---
title: "Research Report"
subtitle: "Big Data Analytics Group Examination: Take-Home Exercises"
author:
- Jonas Paul Schmitten (20-620-894)
- Noah Julian Angara (16-604-233)
date: "14/06/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Group examination: Step-by-step implementation of data pipeline (max. 30 points)

## Task 1: Data gathering

### Solution

```{r message=FALSE, warning=FALSE}

#required libraries
library(xml2)
library(httr)
library(rvest)
library(dplyr)
library(readr)
library(vroom)
library(parallel)
library(pryr)
library(R.utils)
library(data.table)

# set working directory according to who is executing the code
Paths = c("/Users/jonasschmitten/Downloads/GitHub/take-home-exercises-team-send-it/data",
          "/Users/noahangara/Documents/GitHub/take-home-exercises-team-send-it/data")
names(Paths) = c("jonasschmitten", "noahangara")
setwd(Paths[Sys.info()[7]])

#get HTML links 
fec_page <- read_html("https://sunlightlabs.github.io/datacommons/bulk_data.html")
zip_links <- fec_page %>% #get all links for the zip files by contribution year
  html_nodes(xpath="//*[@id='content']/ul[1]") %>% html_nodes("a") %>%
  html_attr("href")

#increase timeout option to download files (default is 60)
getOption("timeout")
options(timeout = 2000)

#download zip files
#RUNTIME : ~9 minutes (depends on network speed) on Macbook Air 2017 i7 8GB RAM
download.file(zip_links, basename(zip_links), method="libcurl")
#unzip and remove zip files
lapply(basename(zip_links), function(x)unzip(x, exdir = getwd()))
file.remove(c(basename(zip_links), "README"))
gc()

#combine csv files into one (only reading one file into memory at a time)
#RUNTIME: ~7 minutes on Macbook Air 2017 i7 8GB RAM & 4 cores (all covariates)
#         ~1.97 minutes on Macbook Air 2017 i7 8GB RAM & 4 cores (relevant columns)
beginning <- Sys.time()
files <- list.files(pattern = "contribution")
for (i in files) {
  d <- vroom(i, num_threads = detectCores(), na = c("", "NA"), col_select = c("cycle", "amount",
                                                                              "contributor_category",
                                                                              "recipient_party",
                                                                              "recipient_name",
                                                                              "recipient_state",
                                                                              "recipient_type",
                                                                              "contributor_category", 
                                                                              "transaction_type"))
  gc()
  first <- i == list.files(pattern = "contribution")[1]
  fwrite(d, "fec.csv", nThread = detectCores(), sep = "\t", quote = F, append = !first, na = NA)
}
ending <- Sys.time()

#keep track of memory used
mem_used()

#compress csv file and remove redundant files
#RUNTIME: ~3.5 minutes on Macbook Air 2017 i7 8GB RAM
gzip("fec.csv", destname = "fec.csv.zip")
file.remove(list.files(pattern = "contributions"))

```

### Exposition of the solution
* Which approach did you take to import CSVs and how does it basically work?

To load in the csv files we made use of the "vroom" package. The vroom package exploits "lazy loading", whereby indices are created where each record is located instead of reading in the whole data set. Therefore, memory is only consumed once the records are needed. This results in much faster performance compared to alternative packages such as data.table or readr. See https://vroom.r-lib.org/articles/benchmarks.html for benchmark results.

* What is the advantage of downloading the data set in individual parts (batches) instead of downloading the one large zip-file containing all observations at once?

Depending on the bandwith your ISP gives you, downloading in batches may proceed faster than downloading one large zip-file. This is especially the case if the download process for the individual parts is parallelized. Furthermore, having individual files gives additional flexibility when loading in only the specific files required for the task vs. all observations (e.g. if the files are split according to year).

* The requirement of using zipping/file compression in the task you have just implemented has consequences for the computational resources needed to run this task. Which hardware component(s) are used more efficiently due to file compression in this task and which ones less efficiently (if any)?

Due to their smaller file size, zip files put less pressure on memory components such as RAM and the hard disk. On the other hand, it takes more processing power (CPU) to unzip files compared to having the files already stored in a decompressed format.

## Task 2: Data storage and databases

### Solution

```{r message=FALSE, warning=FALSE}

#required libraries
library(RSQLite)
library(htmltab)
library(R.utils)

# set working directory according to who is executing the code
Paths = c("/Users/jonasschmitten/Downloads/GitHub/take-home-exercises-team-send-it/data",
          "/Users/noahangara/Documents/GitHub/take-home-exercises-team-send-it/data")
names(Paths) = c("jonasschmitten", "noahangara")
setwd(Paths[Sys.info()[7]])

#Download additional data
transactions <- htmltab(doc = "https://www.fec.gov/campaign-finance-data/transaction-type-code-descriptions/"
                        , which = '//*[@id="main"]/article/div/div/div/div[2]/div/table')
colnames(transactions) <- c("Type", "Description")

industrycodes <- read.csv("http://assets.transparencydata.org.s3.amazonaws.com/docs/catcodes.csv")

#unzip fec.csv if it exists (allows to run task 2 multiple times without running task 1 again)
if (file.exists('fec.csv.zip'))
   R.utils:::gunzip("fec.csv.zip", destname = "fec.csv")

#Create SQLite database called fec.sqlite
con <- dbConnect(RSQLite::SQLite(), "fec.sqlite")

#Create table for transaction types
dbWriteTable(con, "transactiontypes", transactions, overwrite = T, field.types = c(
  Type = "varchar(3)",
  Description = "text"))

#Create table for industry 
dbWriteTable(con, "industrycodes", industrycodes, overwrite = T, field.types = c(
  source = "varchar(3)",
  code = "varchar(5)",
  name = "varchar(96)",
  industry = "varchar(50)",
  order = "varchar(3)"))

#Create table for donations
#RUNTIME: ~1.65 minutes on Macbook Air 2017 i7 8GB RAM & 4 cores
beginning_query <- Sys.time()
dbWriteTable(con, "donations", "fec.csv", sep = "\t", overwrite = T, field.types = c(
  cycle = "int",
  amount = "real",
  contributor_category = "varchar(5)",
  recipient_party = "varchar(3)",
  recipient_name = "varchar(50)",
  recipient_state = "varchar(2)",
  recipient_type = "varchar(3)",
  transaction_type = "varchar(3)"
))
end_query <- Sys.time()

#Create index for those variables needed in later analysis (makes retrieval more efficient)
#For transaction type data
invisible(dbExecute(con, 'CREATE INDEX index_transaction ON transactiontypes (Type,Description);'))

#For industry code data
invisible(dbExecute(con, 'CREATE INDEX index_industry ON industrycodes (source, code, name, industry);'))

#For transaction data
invisible(dbExecute(con, 'CREATE INDEX index_donation ON donations (amount, cycle, contributor_category, recipient_party, recipient_name, recipient_state, recipient_type);'))

#show difference in memory usage of an integer vs string
object.size(42)
object.size("42")

# Disconnect from the database
dbDisconnect(con)
```

### Exposition of the solution

* In the exposition of your solution, explain first why it makes sense to keep the three data sets in three different tables.

It makes sense to keep the data in separate tables to comply with the normal forms of relational database design proposed by Edgar Codd. The purpose of these normal forms is to remove any redundancies or duplicates. If we merge the donations table with the industrycodes and transactiontypes table the keys (e.g. industry code and FEC transaction type) are duplicated, as they are present in all three tables. 

* Then explain what you did to optimize the database and (in simple terms) why your optimization improves the database in comparison to the same database without any optimization (all columns as TEXT and no indices).

To optimize the database we made use of indices and storage classes. Similar to an index at the back of a book, SQLite can find the data quicker with the help of indices, as it does not have to scan the entire table but rather specific columns (although we do compromise on storage space). Storage classes can be used to optimize the amount of disk space used by the columns. For example, storing the number 42 as text requires 112 bytes, while storing it as an integer would only require 56 bytes. This is because integers are between 2 and 8 bytes in size (usually 2 bytes if between -32'768 and 32'768), while text is at least 4 bytes + the number of characters. 

* Is it faster? Does it use less storage space? why?

Using the sqlite3_analyzer.exe downloaded from https://www.sqlite.org/sqlanalyze.html and running "sqlite3_analyzer /Users/noahangara/Documents/GitHub/take-home-exercises-team-send-it/data/fec.sqlite" from the terminal we can compare the storage size of the donations table with and without specifying the storage class. Specifying the storage classes of cycle and amount results in a size of 1'422'925'824 bytes vs. 1'518'264'320 bytes without specifying the storage class. Furthermore, we can time a simple query with and without using indices. Without indices the first query in task 3 took 5.97 minutes, while it only took 41 seconds with indices.

## Task 3: Data aggregation and visualization

### Solution

```{r message=FALSE, warning=FALSE, results= 'asis'}

#required libraries
library(RSQLite)
library(ggplot2)
library(dplyr)
library(ggthemes)
library(knitr)
library(reshape2)
library(kableExtra)

# set working directory according to who is executing the code
Paths = c("/Users/jonasschmitten/Downloads/GitHub/take-home-exercises-team-send-it/data",
          "/Users/noahangara/Documents/GitHub/take-home-exercises-team-send-it/data")
names(Paths) = c("jonasschmitten", "noahangara")
setwd(Paths[Sys.info()[7]])

#connect to fec.sqplite database
con <- dbConnect(RSQLite::SQLite(), "fec.sqlite")

#query all donations from OIL & GAS industry where donation amount is positive
oil_and_gas <- dbGetQuery(con, "SELECT amount, cycle, transaction_type
          FROM donations
          INNER JOIN industrycodes
          ON donations.contributor_category = industrycodes.code
          WHERE industrycodes.industry = 'OIL & GAS'
          AND donations.amount >0")

#query sum of total donations by election cycle
total_contributions <- dbGetQuery(con, "SELECT cycle, 
                                  SUM (amount) 
                                  FROM donations
                                  GROUP BY cycle")

#get memory size of to see if it is manageable with dplyr
print(c(object.size(oil_and_gas), object.size(total_contributions)))

#data cleaning dataframe with total and relative contributions
contribution_plot <- data.frame(
  absolute = oil_and_gas %>% group_by(cycle) %>%summarise(absolute = sum(amount)),
  relative = oil_and_gas %>% group_by(cycle) %>%summarise(absolute = sum(amount))/total_contributions$`SUM (amount)`*100) %>%
  select(c(1,2,4))
colnames(contribution_plot) <- c("Cycle", "Absolute", "Relative (%)")
#for proper format into ggplot
contribution_plot <- melt(contribution_plot, "Cycle")

#create bar plot with relative and absolute donations from OIL & GAS
#removes scientific notation for y-axis
options(scipen=999) 
ggplot(data = contribution_plot) + 
  geom_bar(aes(x = Cycle, y = value, fill = variable), stat = "identity", show.legend = FALSE) + 
  facet_grid(variable ~ ., scales='free') + xlab("Cycle") + theme_fivethirtyeight()

#query total donations by cycle, candidates and from OIL & GAS industry
candidate_donations <- dbGetQuery(con, "SELECT SUM(amount) AS total_amount,
                          cycle, recipient_name
                          FROM donations
                          INNER JOIN industrycodes
                          ON donations.contributor_category = industrycodes.code
                          WHERE industrycodes.industry = 'OIL & GAS' AND recipient_type = 'P' AND amount > 0
                          GROUP BY cycle, recipient_name")

# Disconnect from the database
dbDisconnect(con)

#get top 5 candidates per election cycle and filter out NA's
top_5_candidates <- candidate_donations %>% 
  group_by(cycle) %>% 
  filter(!(recipient_name == "NA")) %>% 
  slice_max(total_amount, n = 5)

#create table with top 5 candidates per election cycle
kable(top_5_candidates, format = "html", caption = "Top Candidates by Donations from Oil & Gas Industry", col.names = c("Amount ", "Cycle ", "Candidate")) %>%
  pack_rows(index = table(top_5_candidates$cycle)) %>% kable_styling()

```

### Exposition of the solution
We first queried the needed data via SQLite to not run into local memory issues and because R dataframes are slower than SQL databases. To do this, we queried the required columns in the first step and then ran operations (sum) in the second step. Since the queried data set was manageable in size, we decided to run dplyr.

## Task 4: Regression analysis

### Solution

```{r message=FALSE, warning=FALSE, results='asis'}

#required libraries
library(RSQLite)
library(stargazer)
library(dplyr)
library(knitr)
library(lfe)
library(parallel)
library(sandwich)

# set working directory according to who is executing the code
Paths = c("/Users/jonasschmitten/Downloads/GitHub/take-home-exercises-team-send-it/data",
          "/Users/noahangara/Documents/GitHub/take-home-exercises-team-send-it/data")
names(Paths) = c("jonasschmitten", "noahangara")
setwd(Paths[Sys.info()[7]])

#connect to fec.sqplite database
con <- dbConnect(RSQLite::SQLite(), "fec.sqlite")

#generate analytic data set
fixed_reg_data <- dbGetQuery(con, "SELECT SUM(amount) AS total_amount,
                          cycle, recipient_name, recipient_party, recipient_state
                          FROM donations
                          INNER JOIN industrycodes
                          ON donations.contributor_category = industrycodes.code
                          WHERE industrycodes.industry = 'OIL & GAS' AND recipient_type = 'P' AND amount > 0
                          GROUP BY cycle, recipient_name")

#create dummy variable for republican
fixed_reg_data <- fixed_reg_data %>% 
  mutate(republican = ifelse(recipient_party == "R", 1, 0)) %>%
  select(-recipient_party)

#MODEL (1)
model1 <- felm(total_amount ~ republican |0|0| recipient_state + cycle, fixed_reg_data)


#MODEL (2)
model2 <- felm(total_amount ~ republican | recipient_state |0| recipient_state + cycle, fixed_reg_data)

#MODEL (3)
model3 <- felm(total_amount ~ republican | recipient_state + cycle |0| recipient_state + cycle, fixed_reg_data)

#present results in table
stargazer(model1, model2, model3, header = F, df = F, title = 'Results Fixed Effects Models', align = T, dep.var.labels = 'Amount', 
          no.space = T, column.labels = c("Model 1", "Model 2", "Model 3"), covariate.labels=c('Republican', 'Intercept'), model.numbers=FALSE, type = 'html', add.lines = list(c("Fixed Effects", "No", "State", "State and Cycle")))

# Disconnect from the database
dbDisconnect(con)
```

### Exposition of the solution

* From what we discussed in the lectures and from what you take from (Gaure 2013), why might the traditional approach to estimate specifications 2 and 3 (with sets of dummy variables for the fixed effects) with OLS (lm-function) be computationally more demanding than the approach implemented in the lfe-package?

To estimate the OLS solution with fixed effects i.e., to demean or remove time-constant effects in the category we have to deal with an $nxn$ matrix $P$, which is a projection on the orthogonal complement of the column space of the dummy variable matrix $D$. However, instead of dealing with the entire matrix $P$ as the lm() function most likely does, Gaure (2013) shows that the matrix can be split into column vectors $P_ix$ according to each factor $i$ in the fixed effects model, making the task computationally less demanding. By parallelizing the computation of the vectors and using more cores the task can be performed more efficiently.

* *Extra point:* motivate briefly why you have chosen to use (which type of) cluster-robust standard errors (or why you have chosen not to use cluster-robust standard errors).

We use cluster-robust standard errors to account for the subdivision of the data into smaller groups (i.e. clusters) where the sampling is correlated within each group. This means that we think that, for instance, being a Republican is highly correlated with the State variable given the prevalence of so-called 'red' and 'blue' states in the US. We account for this because the standard errors are most likely smaller than regular OLS standard errors as a result of clustering. Furthermore, observations within groups can be correlated over time, leading to wrong standard errors if not corrected for. We decided to include cluster-robust standard errors for both State and Cycle in all models following Abadie et. al (2017) because they show that the inclusion of fixed effects is not sufficient to account for clustering. In addition, we decided not to bootstrap the standard errors with groups less than 50 due to simplicity. The outcomes would not have changed meaningfully regardless.



